{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book 6 (Learning to Classify Text)\n",
    "#### SIDE-39-GAB\n",
    "#### Bastomy - 1301178418 - Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Learning to Classify Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1   Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"supervised-classification.png\" style=\"width:500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada pemograman ini kita akan mengklasifikasikan gender berdasarkan nama berikut caranya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1   Gender Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fungsi dibawah berfungsi untuk mengembalikan huruf terakhir dari kata inputan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'y'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features(\"bastomy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "import random\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "berikut daftar nama yang tersedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terdapat  7944 daftar nama\n"
     ]
    }
   ],
   "source": [
    "data_nama = pd.DataFrame(labeled_names)\n",
    "print(\"terdapat \",len(data_nama), \"daftar nama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rebecka</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carol-Jean</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michaella</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lavinie</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anabel</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lester</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gretel</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Halie</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Adeline</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Terrel</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aubrey</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Casia</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hazel</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Germaine</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Gregorio</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Skippie</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hernando</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Pru</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Klee</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sampson</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1\n",
       "0      Rebecka  female\n",
       "1   Carol-Jean  female\n",
       "2    Michaella  female\n",
       "3      Lavinie  female\n",
       "4       Anabel  female\n",
       "5       Lester    male\n",
       "6       Gretel  female\n",
       "7        Halie  female\n",
       "8      Adeline  female\n",
       "9       Terrel    male\n",
       "10      Aubrey  female\n",
       "11       Casia  female\n",
       "12       Hazel    male\n",
       "13    Germaine    male\n",
       "14    Gregorio    male\n",
       "15     Skippie    male\n",
       "16    Hernando    male\n",
       "17         Pru  female\n",
       "18        Klee    male\n",
       "19     Sampson    male"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nama[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Neo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(gender_features('Neo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "klasifikasi diatas sangat case sensitif contoh antara NeO dan Neo menghasilkan hasil yang berbeda, karena klasifikasi di atas hanya memperhatikan huruf terakhir <br> berikut fitur yang tersedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'n'}, 'female'),\n",
       " ({'last_letter': 'a'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'r'}, 'male'),\n",
       " ({'last_letter': 'l'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'e'}, 'female'),\n",
       " ({'last_letter': 'l'}, 'male')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "akurasi klasifikasi sekitar 71 persen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "berikut akhiran nama yang memiliki probabilitas yang tinggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     34.4 : 1.0\n",
      "             last_letter = 'k'              male : female =     31.9 : 1.0\n",
      "             last_letter = 'v'              male : female =     18.8 : 1.0\n",
      "             last_letter = 'f'              male : female =     17.4 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2   Choosing The Right Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fungsi diatas akan mengecek inputan nama dari a-z dan menghitung setiap huruf tersebut, berikut contoh dari hasil fungsi di atas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_letter': 'b',\n",
       " 'last_letter': 'y',\n",
       " 'count(a)': 1,\n",
       " 'has(a)': True,\n",
       " 'count(b)': 1,\n",
       " 'has(b)': True,\n",
       " 'count(c)': 0,\n",
       " 'has(c)': False,\n",
       " 'count(d)': 0,\n",
       " 'has(d)': False,\n",
       " 'count(e)': 0,\n",
       " 'has(e)': False,\n",
       " 'count(f)': 0,\n",
       " 'has(f)': False,\n",
       " 'count(g)': 0,\n",
       " 'has(g)': False,\n",
       " 'count(h)': 0,\n",
       " 'has(h)': False,\n",
       " 'count(i)': 0,\n",
       " 'has(i)': False,\n",
       " 'count(j)': 0,\n",
       " 'has(j)': False,\n",
       " 'count(k)': 0,\n",
       " 'has(k)': False,\n",
       " 'count(l)': 0,\n",
       " 'has(l)': False,\n",
       " 'count(m)': 1,\n",
       " 'has(m)': True,\n",
       " 'count(n)': 0,\n",
       " 'has(n)': False,\n",
       " 'count(o)': 1,\n",
       " 'has(o)': True,\n",
       " 'count(p)': 0,\n",
       " 'has(p)': False,\n",
       " 'count(q)': 0,\n",
       " 'has(q)': False,\n",
       " 'count(r)': 0,\n",
       " 'has(r)': False,\n",
       " 'count(s)': 1,\n",
       " 'has(s)': True,\n",
       " 'count(t)': 1,\n",
       " 'has(t)': True,\n",
       " 'count(u)': 0,\n",
       " 'has(u)': False,\n",
       " 'count(v)': 0,\n",
       " 'has(v)': False,\n",
       " 'count(w)': 0,\n",
       " 'has(w)': False,\n",
       " 'count(x)': 0,\n",
       " 'has(x)': False,\n",
       " 'count(y)': 1,\n",
       " 'has(y)': True,\n",
       " 'count(z)': 0,\n",
       " 'has(z)': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features2('Bastomy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features2(n), gender) for (n, gender) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.754\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pemilihan fitur seperti di atas meningkatkan akurasi sebesar 3 persen yaitu dari 71 ke 74 <br> hal ini di karenakan ada penambahan fitur yaitu menghitung setiap huruf dari a sampai dengan z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = labeled_names[1500:]\n",
    "devtest_names = labeled_names[500:1500]\n",
    "test_names = labeled_names[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"corpus-org.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data corpus dibagi menjadi 3 bagian yaitu train,dev dan tes name seperti pada gambar diatas dengan pembagian data <br>\n",
    "<ul>\n",
    "    <li>train_names 1500 data pertama</li>\n",
    "    <li>devtest_names 500 sampai 1500 data </li>\n",
    "    <li>test_names 500 data terakhir</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "test_set = [(gender_features(n), gender) for (n, gender) in test_names]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dengan pembagian data tersebut mendapatkan akurasi yang meningkat yaitu sebesar 76 persen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'suffix1': word[-1:],\n",
    "            'suffix2': word[-2:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.768\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3   Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "berbeda dengan klasifikasi nama di atas disini yang akan di klasifikasi adalah documen berikut beberapa sample klasifikasi dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cara kerja klasifikasi dokumen ini yaitu dengan mengecek setiap kata yang terkandung dalam dokumen tersebut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = document_features(movie_reviews.words('pos/cv957_8737.txt'))\n",
    "data = list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampel dari ektrasi featur fungsi diatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contains(plot)',\n",
       " 'contains(:)',\n",
       " 'contains(two)',\n",
       " 'contains(teen)',\n",
       " 'contains(couples)',\n",
       " 'contains(go)',\n",
       " 'contains(to)',\n",
       " 'contains(a)',\n",
       " 'contains(church)',\n",
       " 'contains(party)']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      " contains(unimaginative) = True              neg : pos    =      8.5 : 1.0\n",
      "        contains(suvari) = True              neg : pos    =      7.1 : 1.0\n",
      "          contains(mena) = True              neg : pos    =      7.1 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =      6.7 : 1.0\n",
      "        contains(shoddy) = True              neg : pos    =      6.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4   Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', 'en', 'al', '?', 'nt', 'be', 'hat', 'st', 'his', 'th', 'll', 'le', 'ce', 'by', 'ts', 'me', 've', \"'\", 'se', 'ut', 'was', 'for', 'ent', 'ch', 'k', 'w', 'ld', '`', 'rs', 'ted', 'ere', 'her', 'ne', 'ns', 'ith', 'ad', 'ry', ')', '(', 'te', '--', 'ay', 'ty', 'ot', 'p', 'nce', \"'s\", 'ter', 'om', 'ss', ':', 'we', 'are', 'c', 'ers', 'uld', 'had', 'so', 'ey']\n"
     ]
    }
   ],
   "source": [
    "print(common_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = brown.tagged_words(categories='news')\n",
    "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17135753356539035"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contoh dari klasifikasi cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IN'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(pos_features('cats'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contoh lain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IN'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(pos_features('dogs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IN'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(pos_features('fish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kita bisa mnegecek pseudocode pada librari classifier nltk dengan menggunakan kode dibawah ini, dimana depth adalah kedalaman dari kode tersebut, berikut depth dari klasifier tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endswith(e) == False: return 'IN'\n",
      "if endswith(e) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pseudocode(depth=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endswith(e) == False: return 'IN'\n",
      "if endswith(e) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pseudocode(depth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endswith(e) == False: return 'IN'\n",
      "if endswith(e) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pseudocode(depth=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endswith(e) == False: return 'IN'\n",
      "if endswith(e) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pseudocode(depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5   Exploiting Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada fungsi di atas kita akan mengekstrak suatufitur dimana terdapat surfix 1 , 2 dan 3, berikut contoh jika fungsi itu di panggil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kalimat yang akan dijadikan parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(brown.sents()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hasil dari return value fungsi di atas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ion', 'prev-word': 'an'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features(brown.sents()[0], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append( (pos_features(untagged_sent, i), tag) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7891596220785678"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6   Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-tag\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"prev-tag\"] = history[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980528511821975\n"
     ]
    }
   ],
   "source": [
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7   Other Methods for Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2   Further Examples of Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1   Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []\n",
    "boundaries = set()\n",
    "offset = 0\n",
    "for sent in sents:\n",
    "    tokens.extend(sent)\n",
    "    offset += len(sent)\n",
    "    boundaries.add(offset-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_features(tokens, i):\n",
    "    return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
    "            'prev-word': tokens[i-1].lower(),\n",
    "            'punct': tokens[i],\n",
    "            'prev-word-is-one-char': len(tokens[i-1]) == 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
    "               for i in range(1, len(tokens)-1)\n",
    "               if tokens[i] in '.?!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dengan menggunakan fitur kata berikutnya dan sebelumnya dan huruf pertama pada kata sebelumnya menghasilkan akurasi yang sangat tinggi yaitu sebesar 93 persen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.936026936026936"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2   Identifying Dialogue Act Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.668\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(dialogue_act_features(post.text), post.get('class'))\n",
    "               for post in posts]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3   Recognizing Textual Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rte to\n",
      "[nltk_data]     C:\\Users\\Bastomy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\rte.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('rte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rte_features(rtepair):\n",
    "    extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "    features = {}\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne'))\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n",
    "extractor = nltk.RTEFeatureExtractor(rtepair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Russia', 'representing', 'was', 'former', 'China', 'terrorism.', 'Soviet', 'SCO', 'republics', 'Organisation', 'binds', 'central', 'Iran', 'Co', 'fight', 'Parviz', 'Davudi', 'at', 'Asia', 'meeting', 'Shanghai', 'association', 'fledgling', 'that', 'four', 'operation', 'together'}\n"
     ]
    }
   ],
   "source": [
    "print(extractor.text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SCO.', 'member', 'China'}\n"
     ]
    }
   ],
   "source": [
    "print(extractor.hyp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(extractor.overlap('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'China'}\n"
     ]
    }
   ],
   "source": [
    "print(extractor.overlap('ne'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member'}\n"
     ]
    }
   ],
   "source": [
    "print(extractor.hyp_extra('word'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4   Scaling Up to Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python menyediakan lingkungan yang sangat baik untuk melakukan pemrosesan teks dasar dan ekstraksi fitur. Namun, itu tidak dapat melakukan perhitungan intensif numerik yang diperlukan oleh metode pembelajaran mesin hampir secepat bahasa tingkat rendah seperti C. Dengan demikian, jika Anda mencoba menggunakan implementasi pembelajaran mesin pure-Python (seperti nltk.NaiveBayesClassifier) pada dataset besar, Anda mungkin menemukan bahwa algoritma pembelajaran membutuhkan waktu dan memori yang tidak masuk akal untuk diselesaikan.<br>\n",
    "\n",
    "Jika Anda berencana untuk melatih pengklasifikasi dengan sejumlah besar data pelatihan atau sejumlah besar fitur, kami sarankan Anda menjelajahi fasilitas NLTK untuk berinteraksi dengan paket pembelajaran mesin eksternal. Setelah paket-paket ini telah diinstal, NLTK dapat secara transparan memanggil mereka (melalui panggilan sistem) untuk melatih model classifier secara signifikan lebih cepat daripada implementasi classifier murni-Python. Lihat halaman web NLTK untuk daftar paket pembelajaran mesin yang direkomendasikan yang didukung oleh NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3   Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1   The Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import brown\n",
    "tagged_sents = list(brown.tagged_sents(categories='news'))\n",
    "random.shuffle(tagged_sents)\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_set, test_set = tagged_sents[size:], tagged_sents[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = brown.fileids(categories='news')\n",
    "size = int(len(file_ids) * 0.1)\n",
    "train_set = brown.tagged_sents(file_ids[size:])\n",
    "test_set = brown.tagged_sents(file_ids[:size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = brown.tagged_sents(categories='news')\n",
    "test_set = brown.tagged_sents(categories='fiction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada data tes set diatas kita menggunakan categori news sebagai train dan fiction sebagai data tes nya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2   Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Accuracy: {:4.2f}'.format(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3   Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"precision-recall.png\" style=\"height:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\t<li><b>True Positif</b> : hasil prediksi positif dan label sebenarnya juga positif</li>\n",
    "\t<li><b>True Negatif</b> : hasil prediksi negatif dan label sebenarnya juga negatif</li>\n",
    "\t<li><b>False Positif</b> : hasil prediksi negatif dan label sebenarnya positif</li>\n",
    "\t<li><b>False Positif</b> : hasil prediksi positif dan label sebenarnya juga negatif</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>Precision</b> : TP/(TP+FP)</li>\n",
    "    <li><b>Recall</b> : TP/(TP+FN)</li>\n",
    "    <li><b> F-Measure</b>:(2 × Precision × Recall) / (Precision + Recall)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4   Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_list(tagged_sents):\n",
    "    return [tag for sent in tagged_sents for (word, tag) in sent]\n",
    "def apply_tagger(tagger, corpus):\n",
    "    return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = tag_list(brown.tagged_sents(categories='editorial'))\n",
    "test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial')))\n",
    "cm = nltk.ConfusionMatrix(gold, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |                                         N                      |\n",
      "    |      N      I      A      J             N             V      N |\n",
      "    |      N      N      T      J      .      S      ,      B      P |\n",
      "----+----------------------------------------------------------------+\n",
      " NN | <11.9%>  0.0%      .   0.2%      .   0.0%      .   0.2%   0.0% |\n",
      " IN |   0.0%  <9.0%>     .      .      .   0.0%      .      .      . |\n",
      " AT |      .      .  <8.6%>     .      .      .      .      .      . |\n",
      " JJ |   1.7%      .      .  <4.0%>     .      .      .   0.0%   0.0% |\n",
      "  . |      .      .      .      .  <4.8%>     .      .      .      . |\n",
      "NNS |   1.4%      .      .      .      .  <3.3%>     .      .   0.0% |\n",
      "  , |      .      .      .      .      .      .  <4.4%>     .      . |\n",
      " VB |   1.0%      .      .   0.0%      .      .      .  <2.4%>     . |\n",
      " NP |   1.0%      .      .   0.0%      .      .      .      .  <1.9%>|\n",
      "----+----------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5   Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation adalah membagi data set kedalam beberapa bagian , sepeti pembagian data tes dan training di bawah <br> dimana data tes akan terus bergerak ke kanan hingga semua data set teruji dan hasil akhirnya akan di rata-ratakan untuk mengambil nilai akhirnya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cross_validation.png\" style=\"height:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4   Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree adalah model prediksi dengan menggunakan struktur pohon atau struktur berhirarki.\n",
    "Pada decision tree akan memiliki banyak kriteria di mana kriteria tersebut menjadi sabang pada pohon,\n",
    "untuk lebih jelasnya dapat di lihat pada gambar di bawah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1_LlUwyaWkUGJFeEYrWN9_gA.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1   Entropy and Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def entropy(labels):\n",
    "    freqdist = nltk.FreqDist(labels)\n",
    "    probs = [freqdist.freq(l) for l in freqdist]\n",
    "    return -sum(p * math.log(p,2) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n",
      "0.8112781244591328\n",
      "1.0\n",
      "0.8112781244591328\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "print(entropy(['male', 'male', 'male', 'male'])) \n",
    "print(entropy(['male', 'female', 'male', 'male']))\n",
    "print(entropy(['female', 'male', 'female', 'male']))\n",
    "print(entropy(['female', 'female', 'male', 'female']))\n",
    "print(entropy(['female', 'female', 'female', 'female'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5   Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing adalah tahap mempersiapkan data sebelum di proses, dimana pada tahap preprocessing ini terdapat beberapa poses yaitu. <br>\n",
    "<ul>\n",
    "\t<li>casefold</li>\n",
    "\t<li>tokenisasi</li>\n",
    "\t<li>stopword removal</li>\n",
    "\t<li>remove symbol dan character</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada tahap ini dibutuhkan beberapa library yang dibutuhkan yaitu.\n",
    "<ul>\n",
    "\t<li><b>nltk.tokenize</b>: berfungsi untuk membuat kalimat tweet menjadi token</li>\n",
    "\t<li><b>Regular Expresion (re)</b> : untuk menghapus karakter dan simbol yang tidak dibutuhkan</li>\n",
    "\t<li><b>nltk.corpus </b> : untuk mendapatkan daftar kata yang tidak memiliki makna</li>\n",
    "\t<li><b>pandas</b> : untuk memproses data yang berhubungan dengan csv</li>\n",
    "\t<li><b>numpy</b> : untuk memproses data yang berhubungan dengan array</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('indonesian'))\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casefold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "untuk merubah kalimat tweet menjadi lowercase (huruf kecil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casefold(str):\n",
    "    return str.casefold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merubah kalimat tweet menjadi sebuah token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisasi(str):\n",
    "    return tknzr.tokenize(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "untuk menghapus kata yang tidak memiliki makna. berikut contoh beberapa kata yang terdapat dalam stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['termasuk', 'ketika', 'sebagaimana', 'pada', 'sana', 'atau', 'karenanya', 'itulah', 'menantikan', 'sekalian', 'amatlah', 'semata-mata', 'jikalau', 'tegasnya', 'sesama', 'memberikan', 'harus', 'menambahkan', 'diperlukannya', 'sekecil', 'usai', 'diberi', 'berarti', 'bakal', 'beginian', 'melakukan', 'waktunya', 'seharusnya', 'per', 'tiap']\n"
     ]
    }
   ],
   "source": [
    "data = list(set(stopWords))\n",
    "print(data[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal(token):\n",
    "    result = []\n",
    "    for i in range(len(token)):\n",
    "        if token[i] not in stopWords: #mengecek apakah token tidak ada dalam stopword\n",
    "            result.append(token[i]) #menyimpan dalam result untuk d return\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada fungsi ini dilakukan tahap penghapusan.<br>\n",
    "<ul>\n",
    "\t<li>mention dan hastag</li>\n",
    "\t<li>Link</li>\n",
    "\t<li>Symbol</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_symbol_removal(token):\n",
    "    new_token = []\n",
    "    for t in token:\n",
    "        if t[:1] != \"@\" and t[:1] != \"#\": # mengecek token yang tidak diawali dengan @ dan #\n",
    "            url_removed = re.sub(r\"http\\S+\", \"\", t) #mengubah http menjadi \"\" agar terhapus\n",
    "            emoji_removed = re.sub(r\"([\\\\x][a-z0-9A-Z]+)\",\"\",url_removed)\n",
    "            symbol_removed = re.sub(r\"[^\\w]\", \"\", emoji_removed)\n",
    "            if symbol_removed != '' and symbol_removed != '\\xF0': # mengecek agar tidak ada token yang kosong nantinya\n",
    "                new_token.append(symbol_removed)\n",
    "    return new_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada tahap ini fungsi di atas digabung menjadi satu agar mudah di panggil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(string):\n",
    "    string = string[2:-1] # string dimulai dari 2 sampai -1 karena data sebelumnya telah di encode\n",
    "    a = casefold(string) # merubah ke huruf kecil\n",
    "    a = tokenisasi(a) # membuat token\n",
    "    a = stopword_removal(a) # menghapus kata yang tidak penting\n",
    "    a = url_symbol_removal(a) # menghapus simbol dan link\n",
    "    a = ' '.join(a) # membuat array menjadi string\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada tahap ini data yang telah di training atau diberi label akan di baca menggunakan pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training_labeled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "berikut sample data dari data training<br>\n",
    "<ul>\n",
    "\t<li>1 = positif</li>\n",
    "\t<li>0 = negatif</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID Tweet</th>\n",
       "      <th>ID User</th>\n",
       "      <th>Screen Name</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'1127125080545189888'</td>\n",
       "      <td>b'1115202813968015360'</td>\n",
       "      <td>SGanjen</td>\n",
       "      <td>b'@katakitatweet @Aryprasetyo85 @emhaainunnadj...</td>\n",
       "      <td>05-11-19 08:15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'1127125040342786050'</td>\n",
       "      <td>b'973292600017895425'</td>\n",
       "      <td>brothe28</td>\n",
       "      <td>b'@RachlanNashidik @prabowo @jokowi Biasa demo...</td>\n",
       "      <td>05-11-19 08:15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'1127125027827044352'</td>\n",
       "      <td>b'1058630527119323136'</td>\n",
       "      <td>Toyezzz1</td>\n",
       "      <td>b'@DiniKurnia21 @MangiranNing @MahesaTiwi @kan...</td>\n",
       "      <td>05-11-19 08:15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'1127125020407320576'</td>\n",
       "      <td>b'1119035387387600896'</td>\n",
       "      <td>Faqih07380284</td>\n",
       "      <td>b'@permadiaktivis @bawaslu_RI @prabowo @DivHum...</td>\n",
       "      <td>05-11-19 08:15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'1127125017613881344'</td>\n",
       "      <td>b'1018171197350035456'</td>\n",
       "      <td>Jusca07538974</td>\n",
       "      <td>b'Makin muak dgn tingkah2 para Pengumpat dari ...</td>\n",
       "      <td>05-11-19 08:15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b'1127125008566837248'</td>\n",
       "      <td>b'887520780354953216'</td>\n",
       "      <td>leenahanwoo</td>\n",
       "      <td>b'@FerdinandHaean2 @jokowi @KPU_ID @prabowo In...</td>\n",
       "      <td>05-11-19 08:15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b'1127124908838871040'</td>\n",
       "      <td>b'1100944001115406336'</td>\n",
       "      <td>sasauw_nelson</td>\n",
       "      <td>b'@FerdinandHaean2 @jokowi @KPU_ID @prabowo HA...</td>\n",
       "      <td>05-11-19 08:14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b'1127124904405377024'</td>\n",
       "      <td>b'437379680'</td>\n",
       "      <td>Rien_Harbani</td>\n",
       "      <td>b'@RachlanNashidik @prabowo @jokowi Klo drmokr...</td>\n",
       "      <td>05-11-19 08:14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b'1127124885279350784'</td>\n",
       "      <td>b'941513645925658624'</td>\n",
       "      <td>beritaemak</td>\n",
       "      <td>b'@FerdinandHaean2 Pa @prabowo gausah turun bi...</td>\n",
       "      <td>05-11-19 08:14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b'1127124880598585345'</td>\n",
       "      <td>b'2321810466'</td>\n",
       "      <td>NiswariSejuk</td>\n",
       "      <td>b'@Demokrat_TV @renandabachtar Apa 02 gk suka ...</td>\n",
       "      <td>05-11-19 08:14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID Tweet                 ID User    Screen Name  \\\n",
       "0  b'1127125080545189888'  b'1115202813968015360'        SGanjen   \n",
       "1  b'1127125040342786050'   b'973292600017895425'       brothe28   \n",
       "2  b'1127125027827044352'  b'1058630527119323136'       Toyezzz1   \n",
       "3  b'1127125020407320576'  b'1119035387387600896'  Faqih07380284   \n",
       "4  b'1127125017613881344'  b'1018171197350035456'  Jusca07538974   \n",
       "5  b'1127125008566837248'   b'887520780354953216'    leenahanwoo   \n",
       "6  b'1127124908838871040'  b'1100944001115406336'  sasauw_nelson   \n",
       "7  b'1127124904405377024'            b'437379680'   Rien_Harbani   \n",
       "8  b'1127124885279350784'   b'941513645925658624'     beritaemak   \n",
       "9  b'1127124880598585345'           b'2321810466'   NiswariSejuk   \n",
       "\n",
       "                                               Tweet       Timestamp  Label  \n",
       "0  b'@katakitatweet @Aryprasetyo85 @emhaainunnadj...  05-11-19 08:15      1  \n",
       "1  b'@RachlanNashidik @prabowo @jokowi Biasa demo...  05-11-19 08:15      0  \n",
       "2  b'@DiniKurnia21 @MangiranNing @MahesaTiwi @kan...  05-11-19 08:15      0  \n",
       "3  b'@permadiaktivis @bawaslu_RI @prabowo @DivHum...  05-11-19 08:15      0  \n",
       "4  b'Makin muak dgn tingkah2 para Pengumpat dari ...  05-11-19 08:15      1  \n",
       "5  b'@FerdinandHaean2 @jokowi @KPU_ID @prabowo In...  05-11-19 08:15      1  \n",
       "6  b'@FerdinandHaean2 @jokowi @KPU_ID @prabowo HA...  05-11-19 08:14      1  \n",
       "7  b'@RachlanNashidik @prabowo @jokowi Klo drmokr...  05-11-19 08:14      1  \n",
       "8  b'@FerdinandHaean2 Pa @prabowo gausah turun bi...  05-11-19 08:14      1  \n",
       "9  b'@Demokrat_TV @renandabachtar Apa 02 gk suka ...  05-11-19 08:14      1  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data[\"Tweet\"]\n",
    "label = data[\"Label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setiap tweet akan looping dan di klasifikasikan menjadi kelas Positif atau Negatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(tweets)):\n",
    "    pre = preprocessing(tweets[i]) if preprocessing(tweets[i]) != \"\" else \"bagus\" if label[i] == 1 else \"payah\"\n",
    "    result.append({'Text' : pre,'Label' : label[i]})\n",
    "df = pd.DataFrame(result) # membuat data frame dari result\n",
    "df.to_csv('preprocessing result.csv', index=False, header='column_names') # mengubah dataframe ke csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data hasil processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>orang disampingnya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>demokratnya mah gk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>wowo anak buah kayak dancok untung kau yg dian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>lucu gue liat d yg dukung orng ndtang d ilc bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>muak dgn tingkah 2 pengumpat kubu puasa ramadh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>insya allah ayahanda tulus jujur jujur membela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>rakyat bodoh dungu sj ndpt dipengaruhi elit po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>klo drmokrat tdk keprntingan lg kah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>pa gausah turun biar emak aja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>02 gk suka tdk menganggp lbih yg kalah jk kalh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "0      1                                 orang disampingnya\n",
       "1      0                                 demokratnya mah gk\n",
       "2      0  wowo anak buah kayak dancok untung kau yg dian...\n",
       "3      0  lucu gue liat d yg dukung orng ndtang d ilc bi...\n",
       "4      1  muak dgn tingkah 2 pengumpat kubu puasa ramadh...\n",
       "5      1  insya allah ayahanda tulus jujur jujur membela...\n",
       "6      1  rakyat bodoh dungu sj ndpt dipengaruhi elit po...\n",
       "7      1                klo drmokrat tdk keprntingan lg kah\n",
       "8      1                      pa gausah turun biar emak aja\n",
       "9      1  02 gk suka tdk menganggp lbih yg kalah jk kalh..."
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('preprocessing result.csv')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada fungsi ini data training akan dihitung frequensi kata untuk melanjutkan ke tahap berikutnya yaitu menghitung probability setiap kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(texts):\n",
    "    token=[]\n",
    "    positive={}\n",
    "    negative={}\n",
    "    for i in range(len(texts)):\n",
    "        t = tokenisasi(texts[i])\n",
    "        for something in t:\n",
    "            if len(something) > 2 and not is_number(something):\n",
    "                if something not in token:\n",
    "                    token.append(something)\n",
    "                    if labels[i] == 1:\n",
    "                        positive[something] = 1\n",
    "                        negative[something] = 0\n",
    "                    else:\n",
    "                        positive[something] = 0\n",
    "                        negative[something] = 1\n",
    "                else:\n",
    "                    if label[i] == 1:\n",
    "                        positive[something] += 1\n",
    "                    else:\n",
    "                        negative[something] += 1\n",
    "    kata = []\n",
    "    positif = []\n",
    "    negatif = []\n",
    "    for key in positive:\n",
    "        kata.append(key)\n",
    "        positif.append(positive[key])\n",
    "        negatif.append(negative[key])\n",
    "    res = pd.DataFrame({\n",
    "        \"kata\" : kata,\n",
    "        \"positif\" : positif,\n",
    "        \"negatif\" : negatif\n",
    "    })\n",
    "    return res,token,positif,negatif,positive,negative    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fungsi dibawah untuk menghitung probabilitas dari setiap kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(w,c,token,positif,negatif,positive,negative):\n",
    "    if c == \"positif\":\n",
    "        if w not in token:\n",
    "            return (0+1)/(sum(positif)+len(token))\n",
    "        else:\n",
    "            return (positive[w]+1)/(sum(positif)+len(token))\n",
    "    elif c == \"negatif\":\n",
    "        if w not in token:\n",
    "            return (0+1)/(sum(negatif)+len(token))\n",
    "        else:\n",
    "            return (negative[w]+1)/(sum(negatif)+len(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fungsi dibawah untuk menghitung probabilitas dari satu tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(text,token,positif,negatif):\n",
    "    words = tokenisasi(text)\n",
    "    positive_probability = np.prod([prob(word,\"positif\",token,positif,negatif,positive,negative) for word in words]+[prior_positif])\n",
    "    negative_probability = np.prod([prob(word,\"negatif\",token,positif,negatif,positive,negative) for word in words]+[prior_negatif])\n",
    "    return \"positif\" if positive_probability > negative_probability else \"negatif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification berfungsi untuk menghitung akurasi dan F1 score dimana hasil prediksi akan dibandungkan dengan data yang telah di beri label, hasil dari fungsi ini akan menghasilkan nilai TP,FP,TN,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classification(length_training,likelihood,testing):\n",
    "    TP,FP,TN,FN =0,0,0,0\n",
    "    precision,recall,F1,accuracy=0,0,0,0\n",
    "    unique, counts = np.unique(testing['label'],return_counts=True)\n",
    "    prior_positif = dict(zip(unique,counts))['positif']/100\n",
    "    prior_negatif = dict(zip(unique,counts))['negatif']/100\n",
    "    result = {\"kalimat\":[],\"label\":[]}\n",
    "    for i in range(len(testing)):\n",
    "        prediction = P(texts[i],token,positif,negatif)\n",
    "        if prediction==\"positif\":\n",
    "            if testing['label'][i]==\"positif\":\n",
    "                TP = TP+1\n",
    "            else:\n",
    "                FP = FP+1\n",
    "        else:\n",
    "            if testing['label'][i]==\"negatif\":\n",
    "                TN = TN+1\n",
    "            else:\n",
    "                FN=FN+1\n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/(TP+FN)\n",
    "    F1=(2*precision*recall)/(precision+recall)\n",
    "    accuracy=(TP+TN)/(TP+FP+TN+FN)\n",
    "    return TP,FP,TN,FN,precision,recall,F1,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing = pd.read_csv('preprocessing result.csv')\n",
    "texts = data_preprocessing[\"Text\"]\n",
    "labels = data_preprocessing[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>orang disampingnya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>demokratnya mah gk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>wowo anak buah kayak dancok untung kau yg dian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>lucu gue liat d yg dukung orng ndtang d ilc bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>muak dgn tingkah 2 pengumpat kubu puasa ramadh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>insya allah ayahanda tulus jujur jujur membela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>rakyat bodoh dungu sj ndpt dipengaruhi elit po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>klo drmokrat tdk keprntingan lg kah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>pa gausah turun biar emak aja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>02 gk suka tdk menganggp lbih yg kalah jk kalh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "0      1                                 orang disampingnya\n",
       "1      0                                 demokratnya mah gk\n",
       "2      0  wowo anak buah kayak dancok untung kau yg dian...\n",
       "3      0  lucu gue liat d yg dukung orng ndtang d ilc bi...\n",
       "4      1  muak dgn tingkah 2 pengumpat kubu puasa ramadh...\n",
       "5      1  insya allah ayahanda tulus jujur jujur membela...\n",
       "6      1  rakyat bodoh dungu sj ndpt dipengaruhi elit po...\n",
       "7      1                klo drmokrat tdk keprntingan lg kah\n",
       "8      1                      pa gausah turun biar emak aja\n",
       "9      1  02 gk suka tdk menganggp lbih yg kalah jk kalh..."
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessing[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memanggil fungsi likekihood untuk menghasilkan data frequency word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "res,token,positif,negatif,positive,negative   = likelihood(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"likelihood.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "berikut sampel data dari hasil likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kata</th>\n",
       "      <th>positif</th>\n",
       "      <th>negatif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>orang</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disampingnya</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demokratnya</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mah</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wowo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anak</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>buah</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kayak</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dancok</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>untung</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kau</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>diancam</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tim</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mawar</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lucu</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gue</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dukung</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>orng</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ndtang</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ilc</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bikin</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>malu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nklo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>dtang</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>debat</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>salah</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mulu</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>nheran</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>muji</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>mahluq</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>nwajar</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>aja</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>biar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>berkembang</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>biak</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>mentok</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>muak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>dgn</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tingkah</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pengumpat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>kubu</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>puasa</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ramadhan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>segan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>dikotori</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>sungguh</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>mohon</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>bantu</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>amankan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            kata  positif  negatif\n",
       "0          orang        1        1\n",
       "1   disampingnya        1        0\n",
       "2    demokratnya        0        1\n",
       "3            mah        0        1\n",
       "4           wowo        0        1\n",
       "5           anak        2        1\n",
       "6           buah        0        1\n",
       "7          kayak        0        1\n",
       "8         dancok        0        1\n",
       "9         untung        0        1\n",
       "10           kau        0        3\n",
       "11       diancam        0        1\n",
       "12           tim        0        1\n",
       "13         mawar        0        1\n",
       "14          lucu        0        1\n",
       "15           gue        0        2\n",
       "16          liat        1        1\n",
       "17        dukung        1        1\n",
       "18          orng        0        1\n",
       "19        ndtang        0        1\n",
       "20           ilc        0        1\n",
       "21         bikin        0        3\n",
       "22          malu        0        2\n",
       "23          nklo        0        1\n",
       "24         dtang        0        1\n",
       "25         debat        0        1\n",
       "26         salah        2        1\n",
       "27          mulu        0        1\n",
       "28        nheran        0        1\n",
       "29          muji        0        2\n",
       "30        mahluq        0        1\n",
       "31        nwajar        0        1\n",
       "32           aja        1        5\n",
       "33          biar        1        1\n",
       "34    berkembang        0        1\n",
       "35          biak        0        1\n",
       "36        mentok        0        1\n",
       "37          muak        1        0\n",
       "38           dgn        4        4\n",
       "39       tingkah        1        0\n",
       "40     pengumpat        1        0\n",
       "41          kubu        2        0\n",
       "42         puasa        2        0\n",
       "43      ramadhan        1        0\n",
       "44         segan        1        0\n",
       "45      dikotori        1        0\n",
       "46       sungguh        1        0\n",
       "47         mohon        1        0\n",
       "48         bantu        1        0\n",
       "49       amankan        1        0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada tahap ini dilakukan penghitungan prior, yaitu kemungkinan persentasi kelas tertentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(labels,return_counts=True)\n",
    "prior_positif = dict(zip(unique,counts))[1]/100\n",
    "prior_negatif = dict(zip(unique,counts))[0]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior positif  0.61\n",
      "prior negatif  0.39\n"
     ]
    }
   ],
   "source": [
    "print(\"prior positif \",prior_positif)\n",
    "print(\"prior negatif \",prior_negatif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada tahap ini dilakukan looping semua data dan dilakukan klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\"kalimat\":[],\"label\":[]}\n",
    "for i in range(len(texts)):\n",
    "    result['kalimat'].append(texts[i])                       \n",
    "    result['label'].append(P(texts[i],token,positif,negatif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"prediction.csv\", index=False, header='column_names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "berikut adalah hasil prediksi dari klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kalimat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>orang disampingnya</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demokratnya mah gk</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wowo anak buah kayak dancok untung kau yg dian...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lucu gue liat d yg dukung orng ndtang d ilc bi...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>muak dgn tingkah 2 pengumpat kubu puasa ramadh...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>insya allah ayahanda tulus jujur jujur membela...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rakyat bodoh dungu sj ndpt dipengaruhi elit po...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>klo drmokrat tdk keprntingan lg kah</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pa gausah turun biar emak aja</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>02 gk suka tdk menganggp lbih yg kalah jk kalh...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jebakan</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>artikan power kekuatan fisik kekuatan moral po...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>njangan offline side n ndarah merah pemakan da...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>senin bang mengadakan buka puasa rumah kertane...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jd gunanya bawaslu mk dkkp</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bentuklah om</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>resiko pembangkangan rakyat semesta mayoritas ...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pokoknya prabowo menang nharus jd presiden n n...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pokeke lawan 2024 yg berani</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gorengan</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pake tolong ditambahin pendukung 02 n n</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ah bodo sih l</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nmassa andaanda nkedaulatan konstitusi terarah...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>alhamdulillah puisi nw munajat akbar 212 dikab...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>murka allah diaceh bencana alam sunami 26 des ...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>menang 2014 curang klo 2019 tingal diam hadapi...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sok bener dik nmau manganulir pemilu 2019 nnaif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gila keracunan kebanyakan kekuasaan kebodohan ...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ahy ngaku kalah krn urutan ketiga</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>emang ahmad dani yg ditahan n</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>penjarakan nmanusia pengangguran politik nhany...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>diceneralisasi pilpres curang kpu curang bohon...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>win win solution maksud loh grey</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>kerennnn om lanjutkan om semangat tersenyum</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>baca menduga pernyataan hasto fitnah mengarah ...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>jujur</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>slah calon uang rakyat menggunakn lembaga nega...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>cc nbang bro</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>cc in ah</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>ga pake ahy bang pideo nya promosi ya</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>tujuan kubu 02 ngusulin pembentukan tpf utk me...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>mengaku kalah ga pakai alasan ngeles ngeles pe...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>tk sutradara tk rela</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>bikin tpf akal akalan yg kalah aja pa jkw gabu...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>persengkolan parpol pendukung prabowo mengalih...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>bagus</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>pengkhianat nyinyirlah malu mengangkang 02</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>indonesia kandung njika biarkan kandung perkos...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>pemarah mudah stroke</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>salut abang mari berjuang bang anak medan kena...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>alhamdulillah nbukannya ntu tanah leluhur bpk ...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>ukri bocorkan 01 dipaksakan menang n n n n n n...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>sampeyan kalah suara coba pakai jurrus cawet a...</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>kali setuju dgn tweet</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>suaranya g kawan</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>mantap</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>dibawah dikecamatan dikawal polisi saksidikeca...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>netral smnjak liat qc n</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>alhamdulillah</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>masjidnya prabowo yg yah</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              kalimat    label\n",
       "0                                  orang disampingnya  positif\n",
       "1                                  demokratnya mah gk  negatif\n",
       "2   wowo anak buah kayak dancok untung kau yg dian...  negatif\n",
       "3   lucu gue liat d yg dukung orng ndtang d ilc bi...  negatif\n",
       "4   muak dgn tingkah 2 pengumpat kubu puasa ramadh...  positif\n",
       "5   insya allah ayahanda tulus jujur jujur membela...  positif\n",
       "6   rakyat bodoh dungu sj ndpt dipengaruhi elit po...  positif\n",
       "7                 klo drmokrat tdk keprntingan lg kah  positif\n",
       "8                       pa gausah turun biar emak aja  positif\n",
       "9   02 gk suka tdk menganggp lbih yg kalah jk kalh...  positif\n",
       "10                                            jebakan  negatif\n",
       "11  artikan power kekuatan fisik kekuatan moral po...  positif\n",
       "12  njangan offline side n ndarah merah pemakan da...  positif\n",
       "13  senin bang mengadakan buka puasa rumah kertane...  positif\n",
       "14                         jd gunanya bawaslu mk dkkp  positif\n",
       "15                                       bentuklah om  positif\n",
       "16  resiko pembangkangan rakyat semesta mayoritas ...  positif\n",
       "17  pokoknya prabowo menang nharus jd presiden n n...  positif\n",
       "18                        pokeke lawan 2024 yg berani  positif\n",
       "19                                           gorengan  positif\n",
       "20            pake tolong ditambahin pendukung 02 n n  positif\n",
       "21                                      ah bodo sih l  negatif\n",
       "22  nmassa andaanda nkedaulatan konstitusi terarah...  negatif\n",
       "23  alhamdulillah puisi nw munajat akbar 212 dikab...  positif\n",
       "24  murka allah diaceh bencana alam sunami 26 des ...  negatif\n",
       "25  menang 2014 curang klo 2019 tingal diam hadapi...  negatif\n",
       "26    sok bener dik nmau manganulir pemilu 2019 nnaif  negatif\n",
       "27  gila keracunan kebanyakan kekuasaan kebodohan ...  negatif\n",
       "28                  ahy ngaku kalah krn urutan ketiga  negatif\n",
       "29                      emang ahmad dani yg ditahan n  negatif\n",
       "..                                                ...      ...\n",
       "70  penjarakan nmanusia pengangguran politik nhany...  negatif\n",
       "71  diceneralisasi pilpres curang kpu curang bohon...  negatif\n",
       "72                   win win solution maksud loh grey  positif\n",
       "73        kerennnn om lanjutkan om semangat tersenyum  positif\n",
       "74  baca menduga pernyataan hasto fitnah mengarah ...  positif\n",
       "75                                              jujur  positif\n",
       "76  slah calon uang rakyat menggunakn lembaga nega...  positif\n",
       "77                                       cc nbang bro  positif\n",
       "78                                           cc in ah  positif\n",
       "79              ga pake ahy bang pideo nya promosi ya  negatif\n",
       "80  tujuan kubu 02 ngusulin pembentukan tpf utk me...  positif\n",
       "81  mengaku kalah ga pakai alasan ngeles ngeles pe...  negatif\n",
       "82                               tk sutradara tk rela  positif\n",
       "83  bikin tpf akal akalan yg kalah aja pa jkw gabu...  negatif\n",
       "84  persengkolan parpol pendukung prabowo mengalih...  positif\n",
       "85                                              bagus  positif\n",
       "86         pengkhianat nyinyirlah malu mengangkang 02  negatif\n",
       "87  indonesia kandung njika biarkan kandung perkos...  positif\n",
       "88                               pemarah mudah stroke  negatif\n",
       "89  salut abang mari berjuang bang anak medan kena...  positif\n",
       "90  alhamdulillah nbukannya ntu tanah leluhur bpk ...  negatif\n",
       "91  ukri bocorkan 01 dipaksakan menang n n n n n n...  negatif\n",
       "92  sampeyan kalah suara coba pakai jurrus cawet a...  negatif\n",
       "93                              kali setuju dgn tweet  positif\n",
       "94                                   suaranya g kawan  negatif\n",
       "95                                             mantap  positif\n",
       "96  dibawah dikecamatan dikawal polisi saksidikeca...  positif\n",
       "97                            netral smnjak liat qc n  positif\n",
       "98                                      alhamdulillah  positif\n",
       "99                           masjidnya prabowo yg yah  positif\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pada tahap ini dilakukan pengujian menggunakan cross validation dengan k-fold = 10 untuk menghasilkan nilai yang stabil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cross_validation.png\" style=\"height:400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidations(kfold,data):\n",
    "    result = {'Fold':[],'TP':[],'FP':[],'TN':[],'FN':[],'precision':[],'recall':[],'F1':[],'accuracy':[]}\n",
    "    fold = round(len(data) / kfold)\n",
    "    for i in range(kfold):\n",
    "        start = i * fold\n",
    "        end = (i + 1) * fold\n",
    "        training_set = {'kalimat': [],'label':[]}\n",
    "        testing_set = {'kalimat': [],'label':[]}\n",
    "\n",
    "        if end > len(data):\n",
    "            end = len(data)\n",
    "        for j in range(len(data)):\n",
    "            if start<= j <= end:\n",
    "                testing_set['kalimat'].append(data['kalimat'][j])\n",
    "                testing_set['label'].append(data['label'][j])\n",
    "            else:\n",
    "                training_set['kalimat'].append(data['kalimat'][j])\n",
    "                training_set['label'].append(data['label'][j])\n",
    "        training = pd.DataFrame(training_set)\n",
    "        testing = pd.DataFrame(testing_set)\n",
    "        res,token,positif,negatif,positive,negative = likelihood(training['kalimat'])\n",
    "        frequencyWord = pd.DataFrame(res)\n",
    "        TP, FP, TN, FN,precision,recall,F1,accuracy = Classification(len(training),frequencyWord,testing)\n",
    "        result['Fold'].append(i+1)\n",
    "        result['TP'].append(TP)\n",
    "        result['FP'].append(FP)\n",
    "        result['TN'].append(TN)\n",
    "        result['FN'].append(FN)\n",
    "        \n",
    "        result['precision'].append(precision)\n",
    "        result['recall'].append(recall)\n",
    "        result['F1'].append(F1)\n",
    "        result['accuracy'].append(accuracy)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_testing = pd.read_csv('prediction.csv')\n",
    "fold =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = crossValidations(10,data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"hasil_cross.csv\", index=False, header='column_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold  TP  FP  TN  FN  precision    recall        F1  accuracy\n",
       "0     1   7   0   4   0   1.000000  1.000000  1.000000  1.000000\n",
       "1     2   6   1   0   4   0.857143  0.600000  0.705882  0.545455\n",
       "2     3   1   6   2   2   0.142857  0.333333  0.200000  0.272727\n",
       "3     4   5   2   1   3   0.714286  0.625000  0.666667  0.545455\n",
       "4     5   4   3   3   1   0.571429  0.800000  0.666667  0.636364\n",
       "5     6   5   2   1   3   0.714286  0.625000  0.666667  0.545455\n",
       "6     7   5   2   4   0   0.714286  1.000000  0.833333  0.818182\n",
       "7     8   5   2   1   3   0.714286  0.625000  0.666667  0.545455\n",
       "8     9   5   2   3   1   0.714286  0.833333  0.769231  0.727273\n",
       "9    10   5   2   2   1   0.714286  0.833333  0.769231  0.700000"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"hasil_cross.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hasil pengujian menggunakan cross-validation menghasilkan nilai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rata-rata F1  0.6861236802413272\n",
      "rata-rata accuracy  0.6262626262626262\n"
     ]
    }
   ],
   "source": [
    "f1,accuracy=0,0\n",
    "for i in range(len(result)):\n",
    "    f1 = f1+result['F1'][i]\n",
    "    accuracy=accuracy+ result['accuracy'][i]\n",
    "print(\"rata-rata F1 \",f1/len(result))\n",
    "print(\"rata-rata accuracy \",accuracy/len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6   Maximum Entropy Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(features) = Σx |in| corpus P(label(x)|features(x)) <br>\n",
    "dimana P(label|features), probabilitas bahwa input yang fitur-fiturnya fitur akan memiliki label label kelas, didefinisikan sebagai:<br>\n",
    "\n",
    "P(label|features) = P(label, features) / Σlabel P(label, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1   The Maximum Entropy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2   Maximizing Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3   Generative vs Conditional Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7   Modeling Linguistic Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8   Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\t<li>\n",
    "\t\tPemodelan data linguistik yang ditemukan dalam korpora dapat membantu kita memahami pola linguistik, dan dapat digunakan untuk membuat prediksi tentang data bahasa baru.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tPengklasifikasi yang diawasi menggunakan korpora pelatihan berlabel untuk membangun model yang memprediksi label suatu input berdasarkan fitur spesifik dari input tersebut.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tKlasifikasi yang diawasi dapat melakukan berbagai macam tugas NLP, termasuk klasifikasi dokumen, penandaan \tbagian-pidato, segmentasi kalimat, identifikasi jenis tindakan dialog, dan menentukan hubungan keterkaitan, dan banyak tugas lainnya.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tSaat melatih classifier terawasi, Anda harus membagi korpus Anda menjadi tiga dataset: set pelatihan untuk membangun model classifier; satu set dev-test untuk membantu memilih dan menyempurnakan fitur-fitur model; dan satu set uji untuk mengevaluasi kinerja model akhir.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tSaat mengevaluasi classifier yang diawasi, penting bahwa Anda menggunakan data baru, yang tidak termasuk dalam pelatihan atau set dev-test. Jika tidak, hasil evaluasi Anda mungkin optimis secara tidak realistis.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tPohon keputusan secara otomatis dibuat bagan alur struktur pohon yang digunakan untuk menetapkan label pada nilai input berdasarkan fitur mereka. Meskipun mereka mudah diinterpretasikan, mereka tidak pandai menangani kasus di mana nilai fitur berinteraksi dalam menentukan label yang tepat.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tDalam pengklasifikasi naif Bayes, setiap fitur berkontribusi secara independen terhadap keputusan label mana yang harus digunakan. Ini memungkinkan nilai fitur untuk berinteraksi, tetapi bisa bermasalah ketika dua atau lebih fitur sangat berkorelasi satu sama lain.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tPengklasifikasi Entropi maksimum menggunakan model dasar yang mirip dengan model yang digunakan oleh Bayes naif; Namun, mereka menggunakan pengulangan iteratif untuk menemukan set bobot fitur yang memaksimalkan probabilitas set pelatihan.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tSebagian besar model yang dibangun secara otomatis dari corpus bersifat deskriptif - mereka memberi tahu kami fitur mana yang relevan dengan pola atau konstruksi tertentu, tetapi mereka tidak memberikan informasi tentang hubungan sebab akibat antara fitur dan pola tersebut.\n",
    "\t</li>\n",
    "</ul>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
